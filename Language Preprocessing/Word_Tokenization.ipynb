{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization**\n",
        "\n",
        "**Word tokenization** is the process of splitting a text string into a list words, called tokens. \n",
        "\n",
        "For example:\n",
        "> \"Hello, World!\"\n",
        "\n",
        "can be tokenized as:\n",
        "> [\"Hello\", \"World\"]\n"
      ],
      "metadata": {
        "id": "LWo5LONqUpBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "\n",
        "This notebook explores the process of tokenizing a corpus of text into words."
      ],
      "metadata": {
        "id": "XRhHsTisUstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Word Tokenization](#scrollTo=LWo5LONqUpBf)\n",
        "\n",
        ">>[Abstract](#scrollTo=XRhHsTisUstv)\n",
        "\n",
        ">>[Common Word Tokenization Issues](#scrollTo=BY5zQNFc3Pbm)\n",
        "\n",
        ">>[Implementation](#scrollTo=msNFmErgeS3I)\n",
        "\n",
        ">>[Tensorflow Tokenizers](#scrollTo=EjOTql9CeRJv)\n",
        "\n",
        ">>>[Whitespace Tokenization](#scrollTo=iPXvWD7JVdrt)\n",
        "\n",
        ">>>[Unicode Script Word Tokenization](#scrollTo=7kXkhhUdhZxP)\n",
        "\n",
        ">>[NLTK Word Tokenizer](#scrollTo=ENIAIDuLka4a)\n",
        "\n",
        ">>[References](#scrollTo=7EjBIibsaFVJ)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "LbXMAXM6UuKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Word Tokenization Issues\n",
        "\n",
        "- **Clitic Contractions**\n",
        "  - I’m, I’ve, I’d, It’s, We’re\n",
        "- **Possessive Marker**\n",
        "  - The Queen of England's crown\n",
        "- **Negative Marker**\n",
        "  - n’t\n",
        "- **Hyphenated Words**\n",
        "  - State-of-the-art\n",
        "- **Multiword Expressions**\n",
        "  - San Francisco\n",
        "- **Abbreviations**\n",
        "  - m.p.h.\n",
        "  - PhD.\n",
        "- **Formatted Data**\n",
        "  - 25/02/1996\n",
        "  - $69.420"
      ],
      "metadata": {
        "id": "BY5zQNFc3Pbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "msNFmErgeS3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "z0oj4HS26x05"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "alf2kDHJ60rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "413288d4-1b08-4206-b105-fe385aa914c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, word_tokenize, wordpunct_tokenize\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Tokenizers\n",
        "\n",
        "Shown below are some of tokenizers provided by TensorFlow Text. String inputs are assumed to be [UTF-8](https://www.tensorflow.org/text/guide/unicode)."
      ],
      "metadata": {
        "id": "EjOTql9CeRJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tokens(tokens):\n",
        "\n",
        "  if isinstance(tokens, list):\n",
        "    for tokenized_sentence in tokens:\n",
        "      for word in tokenized_sentence:\n",
        "        print(word)\n",
        "      print()\n",
        "\n",
        "  else:\n",
        "    for tokenized_sentence in tokens.to_list():\n",
        "      for word in tokenized_sentence:\n",
        "        print(word.decode())\n",
        "      print()"
      ],
      "metadata": {
        "id": "ZRBrFxLth4oo"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I'm enjoying NLP!\",\n",
        "    \"I would like to work on state-of-the-art NLP research.\",\n",
        "    \"I have never been to San Francisco.\",\n",
        "    \"I might visit U.S.A. in the future.\",\n",
        "    \"My birthday is on 25/02.\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "FUowJ_L5kqnv"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Whitespace Tokenization\n",
        "\n",
        "Whitespace tokenization is the most intuitive way to split text, as it splits the string on whitespace. \n"
      ],
      "metadata": {
        "id": "iPXvWD7JVdrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate a WhitespaceTokenizer\n",
        "whitespace_tokenizer = tf_text.WhitespaceTokenizer()\n",
        "\n",
        "# Tokenize the sentences and show the result\n",
        "tokens = whitespace_tokenizer.tokenize(sentences)\n",
        "print_tokens(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRAYvKqTYGVn",
        "outputId": "670c596e-e921-4809-89c5-cd7632ccc7a6"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm\n",
            "enjoying\n",
            "NLP!\n",
            "\n",
            "I\n",
            "would\n",
            "like\n",
            "to\n",
            "work\n",
            "on\n",
            "state-of-the-art\n",
            "NLP\n",
            "research.\n",
            "\n",
            "I\n",
            "have\n",
            "never\n",
            "been\n",
            "to\n",
            "San\n",
            "Francisco.\n",
            "\n",
            "I\n",
            "might\n",
            "visit\n",
            "U.S.A.\n",
            "in\n",
            "the\n",
            "future.\n",
            "\n",
            "My\n",
            "birthday\n",
            "is\n",
            "on\n",
            "25/02.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unicode Script Word Tokenization\n",
        "\n",
        "The `UnicodeScriptTokenizer` splits strings based on Unicode script boundaries. In practice, this is similar to the `WhitespaceTokenizer` with the most apparent difference being that it will split punctuation.\n"
      ],
      "metadata": {
        "id": "7kXkhhUdhZxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate a UnicodeScriptTokenizer\n",
        "unicode_script_tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "\n",
        "# Tokenize the sentences and show the result\n",
        "tokens = unicode_script_tokenizer.tokenize(sentences)\n",
        "print_tokens(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsSRKj1JhZeR",
        "outputId": "2f910903-6441-434d-f1ac-b15d08bfcc1b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "'\n",
            "m\n",
            "enjoying\n",
            "NLP\n",
            "!\n",
            "\n",
            "I\n",
            "would\n",
            "like\n",
            "to\n",
            "work\n",
            "on\n",
            "state\n",
            "-\n",
            "of\n",
            "-\n",
            "the\n",
            "-\n",
            "art\n",
            "NLP\n",
            "research\n",
            ".\n",
            "\n",
            "I\n",
            "have\n",
            "never\n",
            "been\n",
            "to\n",
            "San\n",
            "Francisco\n",
            ".\n",
            "\n",
            "I\n",
            "might\n",
            "visit\n",
            "U\n",
            ".\n",
            "S\n",
            ".\n",
            "A\n",
            ".\n",
            "in\n",
            "the\n",
            "future\n",
            ".\n",
            "\n",
            "My\n",
            "birthday\n",
            "is\n",
            "on\n",
            "25/02.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK Word Tokenizer"
      ],
      "metadata": {
        "id": "ENIAIDuLka4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlJjfzZfRqPp",
        "outputId": "a026277c-f081-45b5-93a5-7e8ca9b3fbd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "'m\n",
            "enjoying\n",
            "NLP\n",
            "!\n",
            "\n",
            "I\n",
            "would\n",
            "like\n",
            "to\n",
            "work\n",
            "on\n",
            "state-of-the-art\n",
            "NLP\n",
            "research\n",
            ".\n",
            "\n",
            "I\n",
            "have\n",
            "never\n",
            "been\n",
            "to\n",
            "San\n",
            "Francisco\n",
            ".\n",
            "\n",
            "I\n",
            "might\n",
            "visit\n",
            "U.S.A.\n",
            "in\n",
            "the\n",
            "future\n",
            ".\n",
            "\n",
            "My\n",
            "birthday\n",
            "is\n",
            "on\n",
            "25/02\n",
            ".\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokens = []\n",
        "for sentence in sentences:\n",
        "  tokens.append(word_tokenize(sentence))\n",
        "\n",
        "print_tokens(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. *Dan Jurafsky and James H. Martin. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft).*\n",
        "\n",
        "2. Tensorflow's [WhitespaceTokenizer](https://www.tensorflow.org/text/api_docs/python/text/WhitespaceTokenizer) and [UnicodeScriptTokenizer](https://www.tensorflow.org/text/api_docs/python/text/UnicodeScriptTokenizer) Documentation.\n",
        "\n",
        "3. NLTK's [WordTokenizer](https://www.nltk.org/api/nltk.tokenize.html)"
      ],
      "metadata": {
        "id": "7EjBIibsaFVJ"
      }
    }
  ]
}