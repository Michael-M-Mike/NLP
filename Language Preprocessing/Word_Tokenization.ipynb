{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization**"
      ],
      "metadata": {
        "id": "LWo5LONqUpBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "\n",
        "This notebook explores the process of tokenizing a corpus of text into words."
      ],
      "metadata": {
        "id": "XRhHsTisUstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Word Tokenization](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=LWo5LONqUpBf)\n",
        "\n",
        ">>[Abstract](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=XRhHsTisUstv)\n",
        "\n",
        ">>[Word Tokenization](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=8csuou_RVRqk)\n",
        "\n",
        ">>[Common Word Tokenization Issues](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=8csuou_RVRqk)\n",
        "\n",
        ">>[Implementation](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=msNFmErgeS3I)\n",
        "\n",
        ">>[Tensorflow Tokenizers](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=EjOTql9CeRJv)\n",
        "\n",
        ">>>[Whitespace Tokenization](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=iPXvWD7JVdrt)\n",
        "\n",
        ">>>[Unicode Script Word Tokenization](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=7kXkhhUdhZxP)\n",
        "\n",
        ">>[NLTK Word Tokenizer](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=ENIAIDuLka4a)\n",
        "\n",
        ">>[References](#folderId=1mMkxJ9aGYkHVPC6LfQ3j0xpAEfTgI_cq&updateTitle=true&scrollTo=7EjBIibsaFVJ)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "LbXMAXM6UuKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenization\n",
        "\n",
        "**Word tokenization** is the process of splitting a text string into a list words, called tokens. \n",
        "\n",
        "For example:\n",
        "> \"Hello, World!\"\n",
        "\n",
        "can be tokenized as:\n",
        "> [\"Hello\", \"World\"]\n",
        "\n",
        "## Common Word Tokenization Issues\n",
        "\n",
        "- **Clitic Contractions**\n",
        "  - I’m, I’ve, I’d, It’s, We’re\n",
        "- **Possessive Marker**\n",
        "  - The Queen of England's crown\n",
        "- **Negative Marker**\n",
        "  - n’t\n",
        "- **Hyphenated Words**\n",
        "  - State-of-the-art\n",
        "- **Multiword Expressions**\n",
        "  - San Francisco\n",
        "- **Abbreviations**\n",
        "  - m.p.h.\n",
        "  - PhD.\n",
        "- **Formatted Data**\n",
        "  - 25/02/1996\n",
        "  - $69.420\n",
        "\n"
      ],
      "metadata": {
        "id": "8csuou_RVRqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "msNFmErgeS3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z0oj4HS26x05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec865ee-19ce-40a0-af2b-bcf99bacf433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "alf2kDHJ60rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cfa11b-0fae-4ea5-f600-407ac7dde8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, word_tokenize, wordpunct_tokenize\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Tokenizers\n",
        "\n",
        "Shown below are some of tokenizers provided by TensorFlow Text. String inputs are assumed to be [UTF-8](https://www.tensorflow.org/text/guide/unicode)."
      ],
      "metadata": {
        "id": "EjOTql9CeRJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tokens(tokens):\n",
        "\n",
        "  if isinstance(tokens, list):\n",
        "    for tokenized_sentence in tokens:\n",
        "      for word in tokenized_sentence:\n",
        "        print(word)\n",
        "      print()\n",
        "\n",
        "  else:\n",
        "    for tokenized_sentence in tokens.to_list():\n",
        "      for word in tokenized_sentence:\n",
        "        print(word.decode())\n",
        "      print()"
      ],
      "metadata": {
        "id": "ZRBrFxLth4oo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I'm enjoying NLP!\",\n",
        "    \"I would like to work on state-of-the-art NLP research.\",\n",
        "    \"I have never been to San Francisco.\",\n",
        "    \"I might visit U.S.A. in the future.\",\n",
        "    \"My birthday is on 25/02.\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "FUowJ_L5kqnv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Whitespace Tokenization\n",
        "\n",
        "Whitespace tokenization is the most intuitive way to split text, as it splits the string on whitespace. \n"
      ],
      "metadata": {
        "id": "iPXvWD7JVdrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate a WhitespaceTokenizer\n",
        "whitespace_tokenizer = tf_text.WhitespaceTokenizer()\n",
        "\n",
        "# Tokenize the sentences and show the result\n",
        "tokens = whitespace_tokenizer.tokenize(sentences)\n",
        "print_tokens(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRAYvKqTYGVn",
        "outputId": "652c92f8-b028-4dfa-aa78-8654422cc36c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm\n",
            "enjoying\n",
            "NLP!\n",
            "\n",
            "I\n",
            "would\n",
            "like\n",
            "to\n",
            "work\n",
            "on\n",
            "state-of-the-art\n",
            "NLP\n",
            "research.\n",
            "\n",
            "I\n",
            "have\n",
            "never\n",
            "been\n",
            "to\n",
            "San\n",
            "Francisco.\n",
            "\n",
            "I\n",
            "might\n",
            "visit\n",
            "the\n",
            "U.S.A.\n",
            "in\n",
            "the\n",
            "future.\n",
            "\n",
            "My\n",
            "birthday\n",
            "is\n",
            "on\n",
            "25/02.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unicode Script Word Tokenization\n",
        "\n",
        "The `UnicodeScriptTokenizer` splits strings based on Unicode script boundaries. In practice, this is similar to the `WhitespaceTokenizer` with the most apparent difference being that it will split punctuation.\n"
      ],
      "metadata": {
        "id": "7kXkhhUdhZxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate a UnicodeScriptTokenizer\n",
        "unicode_script_tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "\n",
        "# Tokenize the sentences and show the result\n",
        "tokens = unicode_script_tokenizer.tokenize(sentences)\n",
        "print_tokens(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsSRKj1JhZeR",
        "outputId": "342ca25a-747b-44de-cf74-19502f422a19"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "'\n",
            "m\n",
            "enjoying\n",
            "NLP\n",
            "!\n",
            "\n",
            "I\n",
            "would\n",
            "like\n",
            "to\n",
            "work\n",
            "on\n",
            "state\n",
            "-\n",
            "of\n",
            "-\n",
            "the\n",
            "-\n",
            "art\n",
            "NLP\n",
            "research\n",
            ".\n",
            "\n",
            "I\n",
            "have\n",
            "never\n",
            "been\n",
            "to\n",
            "San\n",
            "Francisco\n",
            ".\n",
            "\n",
            "I\n",
            "might\n",
            "visit\n",
            "the\n",
            "U\n",
            ".\n",
            "S\n",
            ".\n",
            "A\n",
            ".\n",
            "in\n",
            "the\n",
            "future\n",
            ".\n",
            "\n",
            "My\n",
            "birthday\n",
            "is\n",
            "on\n",
            "25/02.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK Word Tokenizer"
      ],
      "metadata": {
        "id": "ENIAIDuLka4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlJjfzZfRqPp",
        "outputId": "8ad585b7-763d-41c8-d6d4-92afe93f9faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "'m\n",
            "enjoying\n",
            "NLP\n",
            "!\n",
            "\n",
            "I\n",
            "would\n",
            "like\n",
            "to\n",
            "work\n",
            "on\n",
            "state-of-the-art\n",
            "NLP\n",
            "research\n",
            ".\n",
            "\n",
            "I\n",
            "have\n",
            "never\n",
            "been\n",
            "to\n",
            "San\n",
            "Francisco\n",
            ".\n",
            "\n",
            "I\n",
            "might\n",
            "visit\n",
            "U.S.A.\n",
            "in\n",
            "the\n",
            "future\n",
            ".\n",
            "\n",
            "My\n",
            "birthday\n",
            "is\n",
            "on\n",
            "25/02\n",
            ".\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokens = []\n",
        "for sentence in sentences:\n",
        "  tokens.append(word_tokenize(sentence))\n",
        "\n",
        "print_tokens(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. *Dan Jurafsky and James H. Martin. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft).*\n",
        "\n",
        "2. Tensorflow's [WhitespaceTokenizer](https://www.tensorflow.org/text/api_docs/python/text/WhitespaceTokenizer) and [UnicodeScriptTokenizer](https://www.tensorflow.org/text/api_docs/python/text/UnicodeScriptTokenizer) Documentation.\n",
        "\n",
        "3. NLTK's [WordTokenizer](https://www.nltk.org/api/nltk.tokenize.html)"
      ],
      "metadata": {
        "id": "7EjBIibsaFVJ"
      }
    }
  ]
}