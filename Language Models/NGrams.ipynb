{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **N-Gram Language Models**"
      ],
      "metadata": {
        "id": "VB-1F_Zy6_Zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "\n",
        "This notebook explores the n-gram language models and their implementation using the NLTK library."
      ],
      "metadata": {
        "id": "evD9oHXk7A1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ],
      "metadata": {
        "id": "kHLvGpIc7B9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[N-Gram Language Models](#scrollTo=VB-1F_Zy6_Zk)\n",
        "\n",
        ">>[Abstract](#scrollTo=evD9oHXk7A1-)\n",
        "\n",
        ">>[Table of Contents](#scrollTo=kHLvGpIc7B9K)\n",
        "\n",
        ">>[Language Modelling](#scrollTo=C4-uXyQb7kL6)\n",
        "\n",
        ">>>[Language Models Evaluation](#scrollTo=cEZX9otO9IdZ)\n",
        "\n",
        ">>[N-Grams](#scrollTo=UD_zm9ae-Mvu)\n",
        "\n",
        ">>[N-Gram Language Models](#scrollTo=NWyAWHWM-Iwq)\n",
        "\n",
        ">>>[Example: 4-gram Language Model](#scrollTo=KlVVhhRQAVkk)\n",
        "\n",
        ">>>[Limitations of N-Gram Language Models](#scrollTo=3shPhibjB6n8)\n",
        "\n",
        ">>[Implementation](#scrollTo=XqYbB-MYDZhz)\n",
        "\n",
        ">>>[Import Libraries](#scrollTo=iF0ketsSDbjW)\n",
        "\n",
        ">>>[Prepare the Dataset](#scrollTo=HxHvxJfIGfOy)\n",
        "\n",
        ">>>[Everygrams Language Model](#scrollTo=O5yNzvaSia6f)\n",
        "\n",
        ">>>[Using the Everygrams Trained Model](#scrollTo=eGrO-UwSkTci)\n",
        "\n",
        ">>[References](#scrollTo=en2LHJOq7Eb-)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "CvNYYA8f7Dpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modelling\n",
        "\n",
        "A **language model** allows us to predict the next most probable word, given a sequence of history words. More formally, given a sequence of words $w_1, ..., w_t$, a language model computes the conditional probability distribution of the next word $w_{t+1}$:\n",
        "\n",
        "$P(w_{t+1} | w_1, ..., w_t)$\n",
        "\n",
        "where $w_{t+1}$ can be any word in a vocabulary $V={w_1, ..., w_{|V|}}$. In this way, language modelling can be viewed as a classification task, as there’s a predefined number of possibilities, and the output is a probability distribution over them.\n",
        "\n",
        "An **alternative** equivalent way of thinking about a language model is as a system which assigns a probability to a piece of text. Given a sequence of $T$ words, a language model assigns a probability $P(w_1, …, w_T)$ to the whole sequence, which can be broken down as follows:\n",
        "\n",
        "$P(w_1, ..., w_T) = P(w_1) P(w_2 | w_1) P(w_3 | w_2, w_1) ...  P(w_T | w_{T-1}, ..., w_1)$\n",
        "\n",
        "In practice, a language model gives the probability of a certain word sequence being “valid”. **Validity** in this context does not refer to grammatical validity. It means that it resembles how people speak (or, to be more precise, write) — which is what the language model learns. \n"
      ],
      "metadata": {
        "id": "C4-uXyQb7kL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language Models Evaluation\n",
        "\n",
        "The standard measurable evaluation metric for language models is perplexity. **Perplexity** is defined as the inverse probability of the corpus, according to the language model (the exponent is just for normalisation purposes, otherwise perplexity would get larger and larger as the corpus gets bigger).\n",
        "\n",
        "$Perplexity = \\prod_{t=1}^{T}(\\frac{1}{P(w_{t+1}|w_t,...,w_1)})^\\frac{1}{T}$\n",
        "\n",
        "It can be shown that perplexity is equal to the exponential of the cross-entropy loss J():\n",
        "\n",
        "$Perplexity=e^{J(\\theta)}$\n"
      ],
      "metadata": {
        "id": "cEZX9otO9IdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Grams\n",
        "\n",
        "By definition, an n-gram is a sequence of $n$ consecutive words.\n",
        "\n",
        "- Unigrams:\t  \n",
        "  - “the”\n",
        "- Bigrams:\n",
        "  - “the students”\n",
        "- Trigrams:\n",
        "  - “the students opened”\n",
        "- 4-grams:\n",
        "  - “the students opened their”\n",
        "\n"
      ],
      "metadata": {
        "id": "UD_zm9ae-Mvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Gram Language Models\n",
        "\n",
        "An n-gram model is a statistical model of language whose **core idea** is to collect statistics about how frequently different n-grams appear in a corpus of text, and use them to predict the next most probable next word.\n",
        "\n",
        "N-grams language models make a simplifying **Markov assumption** that the next word $w_{t+1}$ depends solely on the preceding $(n-1)$ words. \n",
        "\n",
        "$P(x_{t+1}|x_t, x_{t-1}, ..., x_1) \\approx P(x_{t+1}|x_t, x_{t-1}, ..., x_{t-n+2})$\n",
        "\n",
        "This probability is computed as follows:\n",
        "\n",
        "$P(x_{t+1}|x_t, x_{t-1}, ..., x_{t-n+2}) = \\frac{P(x_{t+1}, x_t, x_{t-1}, ..., x_{t-n+2})}{P(x_t, x_{t-1}, ..., x_{t-n+2})} = \\frac{Probability \\; of \\; the \\; n-gram}{Probability \\; of \\; the \\; (n-1)-gram}$\n",
        "\n",
        "These n-gram probabilities are obtained by counting them in some large corpus of text (statistical approximation).\n",
        "\n",
        "$\\frac{Probability \\; of \\; the \\; n-gram}{Probability \\; of \\; the \\; (n-1)-gram} = \\frac{Count(x_{t+1}, x_t, x_{t-1}, ..., x_{t-n+2})}{Count(x_t, x_{t-1}, ..., x_{t-n+2})}$"
      ],
      "metadata": {
        "id": "NWyAWHWM-Iwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: 4-gram Language Model\n",
        "\n",
        "> As the proctor started the clock, the students opened their __________.\n",
        "\n",
        "With a 4-gram language model, the next word depends solely on the previous 3 words:\n",
        "\n",
        "> <font color=\"darkred\">~~As the proctor started the clock, the~~</font> <font color=\"royalblue\">students opened their</font> __________.\n",
        "\n",
        "$P(w|students \\; opened \\; their)= \\frac{count(students \\; opened \\; their \\; w)}{count(students \\; opened \\; their)}$\n",
        "\n",
        "Suppose that in the corpus:\n",
        "- “students opened their” occurred 1000 times.\n",
        "- “students opened their books” occurred 400 times: \n",
        "  $P(books|students \\; opened \\; their)=0.4$\n",
        "\n",
        "- “students opened their exams” occurred 100 times.\n",
        "  $P(exams|students \\; opened \\; their)=0.1$\n",
        "\n",
        "Here we can notice the **problem with the Markov assumption**. By discarding the “proctor” context, which implies that the students are in an examination scenario, the prediction isn’t as good as it would’ve been if we kept the sentence’s context."
      ],
      "metadata": {
        "id": "KlVVhhRQAVkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations of N-Gram Language Models\n",
        "\n",
        "- As shown in the example above, **the Markov assumption problem** (it doesn’t take into account contexts that are farther than (n-1) words).\n",
        "\n",
        "- **The sparsity problem**\n",
        "  - If the n-gram never occurs in the corpus (numerator is zero). This is a problem because some words might be uncommon but they’re still valid and thus should have a non-zero probability.\n",
        "\n",
        "    This problem is solved by adding a small smoothing factor to the count for every word in the vocabulary.\n",
        "\n",
        "  - If the (n-1)-gram never occurs in the corpus (denominator is zero). In this case, we can’t even compute the probability distribution at all, for any word.\n",
        "\n",
        "    A possible solution is to back-off to an (n-1)-gram LM.\n",
        "\n",
        "- **Increasing n** \n",
        "  - Makes sparsity problems worse.\n",
        "  - Increases the size of the model (to store the count for all the n-grams).\n",
        "\n",
        "- **The lexical semantic problem** (it treats words as atomic units. Thus, there is no notion of similarity between words). This contributes to the sparsity problem.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3shPhibjB6n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "This section uses NLTK's Language Model Module to implement a trigram language model."
      ],
      "metadata": {
        "id": "XqYbB-MYDZhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "iF0ketsSDbjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends, flatten, padded_everygram_pipeline\n",
        "from nltk.util import ngrams, everygrams\n",
        "from nltk.lm import MLE\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "o5cJeS3ZUumV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Dataset\n",
        "\n",
        "This section prepares the corpus by putting it in the right format expected by trigram language model. This format consists of tokenizing the corpus into a list of sentences, each of which is to be tokenized into a list of words. At which point, a preprocessing pipeline gets a ready-for-training corpus and vocabulary as explained below."
      ],
      "metadata": {
        "id": "HxHvxJfIGfOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A toy corpus is used\n",
        "corpus = [\n",
        "    \"I really like doing nlp projects.\",\n",
        "    \"Music is one of my passions.\",\n",
        "    \"nlp is one of my passions.\",\n",
        "    \"I love music.\"\n",
        "]"
      ],
      "metadata": {
        "id": "HABCVTPETwr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To indicate that some words start and end sentences, special padding symbols are added to the sentence before splitting it into ngrams. This is done using NLTK's `pad_both_ends` function, which adds a `<s>` symbol at the start of the sentence and a `</s>` at its end. Additionally, to make the model more robust, we train it on unigrams (single words) as well as bigrams and trigrams. NLTK once again helpfully provides a function called `everygrams`. While not the most efficient, it is conceptually simple.\n",
        "\n",
        "Finally, during training and evaluation, the model will rely on a vocabulary that defines which words are “known” to the model. To create this vocabulary we need to pad our sentences and then combine the sentences into one flat stream of words."
      ],
      "metadata": {
        "id": "7CVhP8ZEUkre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "\n",
        "  # Lowercasing\n",
        "  text = text.lower()\n",
        "\n",
        "  # Keep only letters\n",
        "  text = re.sub(\"[^a-z ]\", \"\", text)\n",
        "\n",
        "  # Splitting\n",
        "  text = text.split(\" \")\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "WKJjmdAgi-xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padded_everygrams_pipeline(corpus, max_n):\n",
        "  \"\"\"\n",
        "  \"I really like doing nlp projects.\",\n",
        "  \"Music is one of my passions.\",\n",
        "  \"nlp is one of my passions.\",\n",
        "  \"I really love music.\"\n",
        "  \"\"\"\n",
        "  # Keep only letters\n",
        "  corpus = [re.sub(\"[^a-z ]\", \"\", sentence.lower()) for sentence in corpus]\n",
        "\n",
        "  # Split the corpus' sentences into words\n",
        "  \"\"\"\n",
        "  ['I', 'really', 'like', 'doing', 'nlp', 'projects.']\n",
        "  ['Music', 'is', 'one', 'of', 'my', 'passions.']\n",
        "  ['nlp', 'is', 'one', 'of', 'my', 'passions.']\n",
        "  ['I', 'really', 'love', 'music.']\n",
        "  \"\"\"\n",
        "  split_corpus = [sentence.split(\" \") for sentence in corpus]\n",
        "\n",
        "  # Pad the corpus with special <s> </s> symbols\n",
        "  \"\"\"\n",
        "  ['<s>', 'I', 'really', 'like', 'doing', 'nlp', 'projects.', '</s>']\n",
        "  ['<s>', 'Music', 'is', 'one', 'of', 'my', 'passions.', '</s>']\n",
        "  ['<s>', 'nlp', 'is', 'one', 'of', 'my', 'passions.', '</s>']\n",
        "  ['<s>', 'I', 'really', 'love', 'music.', '</s>']\n",
        "  \"\"\"\n",
        "  padded_corpus = [list(pad_both_ends(split_corpus[i], n=2))\n",
        "  for i in range(len(split_corpus))]\n",
        "\n",
        "  # Get unigrams, bigrams, ..., up to max_n-grams for the padded corpus\n",
        "  \"\"\"\n",
        "  ('<s>',)\n",
        "  ('<s>', 'I')\n",
        "  ('<s>', 'I', 'really')\n",
        "  ('I',)\n",
        "  ('I', 'really')\n",
        "  ('I', 'really', 'like')\n",
        "  \"\"\"\n",
        "  padded_everygrams = [list(everygrams(padded_sentence, max_len=max_n))\n",
        "  for padded_sentence in padded_corpus]\n",
        "\n",
        "  # Get the vocabulary\n",
        "  \"\"\"\n",
        "  ['<s>', 'I', 'really', 'like', 'doing', 'nlp', 'projects.', '</s>', \n",
        "  '<s>', 'Music', 'is', 'one', 'of', 'my', 'passions.', '</s>', \n",
        "  '<s>', 'nlp', 'is', 'one', 'of', 'my', 'passions.', '</s>', \n",
        "  '<s>', 'I', 'really', 'love', 'music.', '</s>']\n",
        "  \"\"\"\n",
        "  vocabulary = set(flatten(padded_sentence\n",
        "  for padded_sentence in padded_corpus))\n",
        "\n",
        "  return padded_everygrams, vocabulary"
      ],
      "metadata": {
        "id": "lcvPKB9GZoFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Everygrams Language Model"
      ],
      "metadata": {
        "id": "O5yNzvaSia6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigrams\n",
        "n = 3\n",
        "\n",
        "# Apply the preprocessing pipeline to get the everygrams and the vocabulary\n",
        "train, vocab = padded_everygrams_pipeline(corpus, 3)\n",
        "\n",
        "# Instanciate a trigram language model\n",
        "lm = MLE(3)\n",
        "\n",
        "# Fit the model on the everygrams and the vocabulary\n",
        "lm.fit(train, vocab)"
      ],
      "metadata": {
        "id": "hY4kBoiogJGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Everygrams Trained Model"
      ],
      "metadata": {
        "id": "eGrO-UwSkTci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "  generated_text = \" \".join(lm.generate(5, text_seed=\"i\"))\n",
        "  print(f\"i {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx_F1VXmisUO",
        "outputId": "dbac1ff4-3cfc-43b6-f445-689d6ec0660f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love music </s> nlp is\n",
            "i really like doing nlp projects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. *Dan Jurafsky and James H. Martin. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft).*\n",
        "\n",
        "2. Stanford CS224n: Natural Language Processing with Deep Learning: [Language Models](https://web.stanford.edu/class/cs224n/index.html#coursework)\n",
        "\n",
        "3. NLTK Documentation: [NLTK Language Modeling Module](https://www.nltk.org/api/nltk.lm.html)"
      ],
      "metadata": {
        "id": "en2LHJOq7Eb-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2u5a5-jvyvmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}